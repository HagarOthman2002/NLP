# -*- coding: utf-8 -*-
"""session 1 NLP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15hkoGpDiTKruTOyttGKp1k_sAcXDxQQg

#part 1 sentence segmentation
"""

paragraph = "HII everyone. welcome to seeion 1 of NLP . i hpe you are enjoying this session "

sentences = paragraph.split('.')
sentences

"""#part 2 Remove punctuation"""

import string  #get the punctuations
punctuation = string.punctuation + "" # u can add any punctuation not in the library
# print(string.punctuation)
text = "my name is Ahmed, i am data scientist, i love coding !!!@"
def remove_punctuation(text):
  text_no_punc = "".join([x for x in text if x not in punctuation])
  return text_no_punc
remove_punctuation(text)

"""#part 3 lower casing"""

text = "my name is Ahmed, i am data scientist, i love coding !!!@"
lower_txt = text.lower()
lower_txt

"""#part 4 word tokenization"""

#nltk : package for tokrnizer
import nltk
nltk.download('punkt') # must be downloaded to use nltk
from nltk.tokenize import word_tokenize

text = "my name is Ahmed, i am data scientist, i love coding !!!@"

tokenz = word_tokenize(text)
print(tokenz)

"""#part 5 stop words"""

#if u solve classefication problem u hae to remove stop words because they mean nothing
from nltk.corpus import stopwords
nltk.download('stopwords') #we have to download stop words first

stop_words = stopwords.words('english')
# stop_words


text = "my name is Ahmed, i am data scientist, i love coding !!!@"

def remove_stopwords(text):
  text_no_stop_words = " ".join([x for x in text if x not in stop_words])
  return text_no_stop_words

  #stop words must come after tokenization
tokenz = word_tokenize(remove_punctuation(text))
remove_stopwords(tokenz)

"""#part 6 steamming"""

from nltk.stem import PorterStemmer

example_words = ['pathon' , 'paythonning' , 'program' , 'programmer','programming' ,'rocks' , 'improved']

stemmer = PorterStemmer()

for word in example_words:
  print(word, ":" , stemmer.stem(word))

"""#part 7 limmatization"""

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
lemmtizar = WordNetLemmatizer()

for word in example_words:
  print(word ," : " , lemmtizar.lemmatize(word , pos = 'v'))

"""#part 8 POS tags

"""

from nltk import pos_tag , pos_tag_sents
nltk.download('averaged_perceptron_tagger')

text = "my name is Ahmed, i am data scientist, i love coding !!!@"
pos_tag(text.split())

"""#part 9 Named Entity Recognation (NER)"""

import spacy
from spacy import displacy
from spacy.cli import download

download('en_core_web_sm')

NER = spacy.load('en_core_web_sm')
text = "my name is Ahmed, i am data scientist, i love coding !!!@ I live in Egypt, iam working at IBM, i have 12$ million, borned at December 23"

doc =NER(text)
displacy.render(doc, style ="ent" , jupyter =True)

"""#part 10 charts"""

!pip install svgling

from nltk import ne_chunk

nltk.download('maxent_ne_chunker')
nltk.download('words')
ne_chunk(pos_tag(text.split()) )

